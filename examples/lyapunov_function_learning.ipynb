{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Lyapunov Function for the Inverted Pendulum\n",
    "\n",
    "Construct and train a parameterized Lyapunov function for the inverted pendulum system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utilities import LyapunovNetwork, InvertedPendulum, balanced_class_weights, binary_cmap, compute_roa, monomials\n",
    "\n",
    "# Nice progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  eps                   = 1e-8,                            # numerical tolerance\n",
    "                  saturate              = True,                            # apply saturation constraints to the control input\n",
    "                  use_zero_threshold    = True,                            # assume the discretization is infinitely fine (i.e., tau = 0)\n",
    "                  pre_train             = True,                            # pre-train the neural network to match a given candidate in a supervised approach\n",
    "                  dpi                   = 150,\n",
    "                  num_cores             = 4,\n",
    "                  num_sockets           = 1,\n",
    "                  tf_checkpoint_path    = \"./tmp/lyapunov_function_learning.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    "Customize the TensorFlow session for the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(OPTIONS.num_cores)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = OPTIONS.num_cores,\n",
    "                        inter_op_parallelism_threads  = OPTIONS.num_sockets,\n",
    "                        allow_soft_placement          = False,\n",
    "                        device_count                  = {'CPU': OPTIONS.num_cores})\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "# Set random seed to reproduce results\n",
    "seed = 1\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "Define the nonlinear and linearized forms of the inverted pendulum dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(180)                     # angular position [rad]\n",
    "omega_max = np.deg2rad(360)                     # angular velocity [rad/s]\n",
    "u_max     = g * m * L * np.sin(np.deg2rad(60))  # torque [N.m], control action\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 2\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# Initialize system class and its linearization\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "dynamics = pendulum.__call__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization and Initial Safe Set\n",
    "\n",
    "Define a uniform discretization, and an initial known safe set as a subset of this discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "num_states = 251\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "state_discretization = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant (tau): {}'.format(tau))\n",
    "\n",
    "# Set initial safe set as a ball around the origin (in normalized coordinates)\n",
    "cutoff_radius    = 0.1\n",
    "initial_safe_set = np.linalg.norm(state_discretization.all_points, ord=2, axis=1) <= cutoff_radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Policy\n",
    "\n",
    "Fix the policy to the LQR solution for the linearized, discretized system, possibly with saturation constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.identity(state_dim).astype(OPTIONS.np_dtype)     # state cost matrix\n",
    "R = np.identity(action_dim).astype(OPTIONS.np_dtype)    # action cost matrix\n",
    "K, P_lqr = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "\n",
    "policy = safe_learning.LinearSystem(- K, name='policy')\n",
    "if OPTIONS.saturate:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-Loop Dynamics Lipschitz Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Policy (linear)\n",
    "L_pol = lambda x: np.linalg.norm(-K, 1)\n",
    "\n",
    "# # Dynamics (linear approximation)\n",
    "L_dyn = lambda x: np.linalg.norm(A, 1) + np.linalg.norm(B, 1) * L_pol(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LQR Lyapunov Candidate\n",
    "\n",
    "Define a Lyapunov candidate function corresponding to the LQR solution for the linearized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lyapunov function corresponding to the LQR policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P_lqr)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P_lqr,))\n",
    "L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize Lyapunov class\n",
    "lyapunov_lqr = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOS Lyapunov Candidate\n",
    "\n",
    "Define a sixth-order polynomial Lyapunov candidate with the solution from SOSTOOLS in MatLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyapunov_sos(x):\n",
    "    Z = monomials(x, 3)\n",
    "    Q = np.array([\n",
    "                [       1.33,    0.00373,   5.933e-9,  -2.75e-10, -4.251e-11,     0.1402,   0.005985,  0.0001077,   0.001279],\n",
    "                [    0.00373,    0.00882,  -2.75e-10, -4.251e-11,  -6.36e-11,   0.005985,  0.0001077,   0.001279,   0.000889],\n",
    "                [   5.933e-9,  -2.75e-10,     0.1402,   0.005985,  0.0001077, -1.847e-10,  4.928e-12, -8.276e-13,  1.789e-12],\n",
    "                [  -2.75e-10, -4.251e-11,   0.005985,  0.0001077,   0.001279,  4.928e-12, -8.276e-13,  1.789e-12, -1.937e-13],\n",
    "                [ -4.251e-11,  -6.36e-11,  0.0001077,   0.001279,   0.000889, -8.276e-13,  1.789e-12, -1.937e-13, -3.181e-13],\n",
    "                [     0.1402,   0.005985, -1.847e-10,  4.928e-12, -8.276e-13,   0.004209, -0.0003288,   5.837e-5,  -3.517e-5],\n",
    "                [   0.005985,  0.0001077,  4.928e-12, -8.276e-13,  1.789e-12, -0.0003288,   5.837e-5,  -3.517e-5,  -3.109e-5],\n",
    "                [  0.0001077,   0.001279, -8.276e-13,  1.789e-12, -1.937e-13,   5.837e-5,  -3.517e-5,  -3.109e-5,   4.497e-6],\n",
    "                [   0.001279,   0.000889,  1.789e-12, -1.937e-13, -3.181e-13,  -3.517e-5,  -3.109e-5,   4.497e-6,   2.013e-5]])\n",
    "    linear_form = np.matmul(Z, Q)\n",
    "    quadratic = np.sum(linear_form * Z, axis=1, keepdims=True)\n",
    "    return quadratic\n",
    "\n",
    "\n",
    "values_sos = lyapunov_sos(state_discretization.all_points).ravel()\n",
    "future = dynamics(state_discretization.all_points, policy(state_discretization.all_points)).eval()\n",
    "future_values_sos = lyapunov_sos(future).ravel()\n",
    "dv_sos = future_values_sos - values_sos\n",
    "\n",
    "order = np.argsort(values_sos)\n",
    "ordered_idx = np.argmax(dv_sos[order][1:] >= 0) + 1\n",
    "max_idx = order[ordered_idx - 1]\n",
    "cmax_sos = values_sos[max_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Lyapunov Candidate\n",
    "\n",
    "Define a parameterized Lyapunov candidate function with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64, 64, 64]\n",
    "activations = [tf.tanh, tf.tanh, tf.tanh]\n",
    "lyapunov_function = LyapunovNetwork(state_dim, layer_dims, activations, OPTIONS.eps)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "grad_lyapunov_function = lambda x: tf.gradients(lyapunov_function(x), x)[0]\n",
    "L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize parameters; need to use the template before parameter variables exist in the TensorFlow graph\n",
    "temp = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "temp = lyapunov_function(temp)\n",
    "session.run(tf.variables_initializer(lyapunov_function.parameters))\n",
    "\n",
    "# Initialize Lyapunov class\n",
    "lyapunov_nn = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics\n",
    "tf_states           = tf.placeholder(OPTIONS.tf_dtype, shape=[None, state_dim], name='states')\n",
    "tf_actions          = policy(tf_states)\n",
    "tf_future_states    = dynamics(tf_states, tf_actions)\n",
    "\n",
    "# Neural network\n",
    "tf_values_nn        = lyapunov_nn.lyapunov_function(tf_states)\n",
    "tf_future_values_nn = lyapunov_nn.lyapunov_function(tf_future_states)\n",
    "tf_dv_nn            = tf_future_values_nn - tf_values_nn\n",
    "tf_threshold        = lyapunov_nn.threshold(tf_states, lyapunov_nn.tau)\n",
    "tf_negative         = tf.squeeze(tf.less(tf_dv_nn, tf_threshold), axis=1)\n",
    "\n",
    "# LQR\n",
    "tf_values_lqr        = lyapunov_lqr.lyapunov_function(tf_states)\n",
    "tf_future_values_lqr = lyapunov_lqr.lyapunov_function(tf_future_states)\n",
    "tf_dv_lqr            = tf_future_values_lqr - tf_values_lqr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Region of Attraction\n",
    "\n",
    "Compute the true largest region of attraction (ROA), denoted as $\\mathcal{S}_\\pi$, by forward-simulating the closed-loop dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_dynamics = lambda x: tf_future_states.eval({tf_states: x})\n",
    "horizon = 500\n",
    "tol = 0.1\n",
    "roa, trajectories = compute_roa(lyapunov_nn.discretization, closed_loop_dynamics, horizon, tol, no_traj=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Pre-Training\n",
    "\n",
    "Pre-train on a spherical Lyapunov function to make sure an initial safe set exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.pre_train:\n",
    "    obj = []\n",
    "    level_states = state_discretization.all_points[initial_safe_set]\n",
    "    \n",
    "    # Spherical candidate\n",
    "    P = 0.1 * np.eye(state_dim)\n",
    "    lyapunov_function      = safe_learning.QuadraticFunction(P)\n",
    "    grad_lyapunov_function = safe_learning.LinearSystem((2 * P,))\n",
    "    L_v                    = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "\n",
    "    # Initialize class\n",
    "    lyapunov_pre = safe_learning.Lyapunov(state_discretization, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "    lyapunov_pre.update_values()\n",
    "    lyapunov_pre.update_safe_set()\n",
    "    \n",
    "    # TensorFlow graph elements\n",
    "    tf_values_pre        = lyapunov_pre.lyapunov_function(tf_states)\n",
    "    tf_future_values_pre = lyapunov_pre.lyapunov_function(tf_future_states)\n",
    "    tf_dv_pre            = tf_future_values_pre - tf_values_pre\n",
    "    \n",
    "    with tf.name_scope('lyapunov_pre_training'):\n",
    "        tf_losses        = tf.abs(tf_values_nn - tf_values_pre) # / tf.stop_gradient(tf_values_pre + OPTIONS.eps)\n",
    "        tf_objective     = tf.reduce_mean(tf_losses, name='objective')\n",
    "        tf_learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "        optimizer        = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "        lyapunov_update  = optimizer.minimize(tf_objective, var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "        \n",
    "        tf_batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "        tf_batch      = tf.random_uniform([tf_batch_size, ], 0, level_states.shape[0], dtype=tf.int32, name='batch_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.pre_train:\n",
    "    # Test set\n",
    "    test_size = int(1e3)\n",
    "    idx       = tf_batch.eval({tf_batch_size: int(1e3)})\n",
    "    test_set  = level_states[idx, :]\n",
    "\n",
    "    feed_dict = {\n",
    "        tf_states:         level_states,\n",
    "        tf_learning_rate:  1e-1,\n",
    "        tf_batch_size:     int(1e3),\n",
    "    }\n",
    "    max_iters = 300\n",
    "\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        idx = tf_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = level_states[idx, :]\n",
    "        session.run(lyapunov_update, feed_dict)\n",
    "\n",
    "        feed_dict[tf_states] = test_set\n",
    "        obj.append(tf_objective.eval(feed_dict))\n",
    "        \n",
    "    lyapunov_nn.update_values()\n",
    "    lyapunov_nn.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIONS.pre_train:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=OPTIONS.dpi)\n",
    "    ax.set_xlabel(r'iteration')\n",
    "    ax.set_ylabel(r'objective')\n",
    "    ax.plot(obj, '.-r')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "Train the parameteric Lyapunov candidate in order to expand the verifiable safe set towards $\\mathcal{S}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TensorFlow checkpoint for the neural network parameters\n",
    "saver = tf.train.Saver(var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "ckpt_path = saver.save(session, OPTIONS.tf_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('roa_classification'):\n",
    "    # Target the safe level set to extend out towards\n",
    "    safe_level = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='c_max')\n",
    "    level_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='level_multiplier')\n",
    "    \n",
    "    # True class labels, converted from Boolean ROA labels {0, 1} to {-1, 1}\n",
    "    roa_labels = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='labels')\n",
    "    class_labels = 2 * roa_labels - 1\n",
    "\n",
    "    # Signed, possibly normalized distance from the decision boundary\n",
    "    decision_distance = safe_level - tf_values_nn\n",
    "    \n",
    "    # Perceptron loss with class weights\n",
    "    class_weights = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='class_weights')\n",
    "    classifier_loss = class_weights * tf.maximum(- class_labels * decision_distance, 0, name='classifier_loss')\n",
    "    \n",
    "    # Enforce decrease constraint with Lagrangian relaxation\n",
    "    lagrange_multiplier = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='lagrange_multiplier')\n",
    "    decrease_loss = roa_labels * tf.maximum(tf_dv_nn, 0) / tf.stop_gradient(tf_values_nn + OPTIONS.eps)\n",
    "        \n",
    "    # Construct objective and optimizer\n",
    "    objective = tf.reduce_mean(classifier_loss + lagrange_multiplier * decrease_loss, name='objective')\n",
    "    learning_rate = tf.placeholder(OPTIONS.tf_dtype, shape=[], name='learning_rate')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_update = optimizer.minimize(objective, var_list=lyapunov_nn.lyapunov_function.parameters)\n",
    "\n",
    "with tf.name_scope('sampling'):\n",
    "    batch_size = tf.placeholder(tf.int32, [], 'batch_size')\n",
    "    idx_range = tf.placeholder(tf.int32, shape=[], name='indices_to_sample')\n",
    "    idx_batch = tf.random_uniform([batch_size, ], 0, idx_range, dtype=tf.int32, name='batch_sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Restore parameter checkpoint, and try training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(session, ckpt_path)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()\n",
    "\n",
    "test_classifier_loss = []\n",
    "test_decrease_loss   = []\n",
    "roa_estimate         = np.copy(lyapunov_nn.safe_set)\n",
    "\n",
    "grid              = lyapunov_nn.discretization\n",
    "c_max             = [lyapunov_nn.feed_dict[lyapunov_nn.c_max], ]\n",
    "safe_set_fraction = [lyapunov_nn.safe_set.sum() / grid.nindex, ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "outer_iters = 20\n",
    "inner_iters = 10\n",
    "horizon     = 100\n",
    "test_size   = int(1e4)\n",
    "\n",
    "feed_dict = {\n",
    "    tf_states:           np.zeros((1, grid.ndim)), # placeholder\n",
    "    safe_level:          1.,\n",
    "    lagrange_multiplier: 1000,\n",
    "    #\n",
    "    level_multiplier:    1.2,\n",
    "    learning_rate:       6e-3,\n",
    "    batch_size:          int(1e3),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current metrics ...')\n",
    "c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "num_safe = lyapunov_nn.safe_set.sum()\n",
    "print('Safe level (c_k): {}'.format(c))\n",
    "print('Safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(num_safe), 100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "\n",
    "for _ in range(outer_iters):\n",
    "    print('Iteration (k): {}'.format(len(c_max)))\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Identify the \"gap\" states, i.e., those between V(c_k) and V(a * c_k) for a > 1\n",
    "    c = lyapunov_nn.feed_dict[lyapunov_nn.c_max]\n",
    "    idx_small = lyapunov_nn.values.ravel() <= c\n",
    "    idx_big = lyapunov_nn.values.ravel() <= feed_dict[level_multiplier] * c\n",
    "    idx_gap = np.logical_and(idx_big, ~idx_small)\n",
    "    \n",
    "    # Forward-simulate \"gap\" states to determine which ones we can add to our ROA estimate\n",
    "    gap_states = grid.all_points[idx_gap]\n",
    "    for _ in range(horizon):\n",
    "        gap_states = tf_future_states.eval({tf_states: gap_states})\n",
    "    gap_future_values = tf_values_nn.eval({tf_states: gap_states})    \n",
    "    roa_estimate[idx_gap] |= (gap_future_values <= c).ravel()\n",
    "    \n",
    "    # Identify the class labels for our current ROA estimate and the expanded level set\n",
    "    target_idx = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set = grid.all_points[target_idx]\n",
    "    target_labels = roa_estimate[target_idx].astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    feed_dict[idx_range] = target_set.shape[0]\n",
    "    \n",
    "    # Test set\n",
    "    idx_test = idx_batch.eval({batch_size: test_size, idx_range: target_set.shape[0]})\n",
    "    test_set = target_set[idx_test]\n",
    "    test_labels = target_labels[idx_test]\n",
    "\n",
    "    # SGD for classification\n",
    "    for _ in tqdm(range(inner_iters)):\n",
    "        # Training step\n",
    "        idx_batch_eval = idx_batch.eval(feed_dict)\n",
    "        feed_dict[tf_states] = target_set[idx_batch_eval]\n",
    "        feed_dict[roa_labels] = target_labels[idx_batch_eval]\n",
    "        feed_dict[class_weights], class_counts = balanced_class_weights(feed_dict[roa_labels].astype(bool))\n",
    "        session.run(training_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Record losses on test set\n",
    "        feed_dict[tf_states] = test_set\n",
    "        feed_dict[roa_labels] = test_labels\n",
    "        feed_dict[class_weights], class_counts = balanced_class_weights(feed_dict[roa_labels].astype(bool))\n",
    "        results = session.run([classifier_loss, decrease_loss], feed_dict)\n",
    "        test_classifier_loss.append(results[0].mean())\n",
    "        test_decrease_loss.append(results[1].mean())\n",
    "\n",
    "    # Update Lyapunov values and ROA estimate, based on new parameter values\n",
    "    lyapunov_nn.update_values()\n",
    "    lyapunov_nn.update_safe_set()\n",
    "    roa_estimate |= lyapunov_nn.safe_set\n",
    "\n",
    "    c_max.append(lyapunov_nn.feed_dict[lyapunov_nn.c_max])\n",
    "    safe_set_fraction.append(lyapunov_nn.safe_set.sum() / grid.nindex)\n",
    "    print('Current safe level (c_k): {}'.format(c_max[-1]))\n",
    "    print('Safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(int(lyapunov_nn.safe_set.sum()), \n",
    "                                                                         100 * safe_set_fraction[-1], \n",
    "                                                                         100 * safe_set_fraction[-1] * roa.size / roa.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3), dpi=OPTIONS.dpi, frameon=False)\n",
    "fig.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plot_limits = np.column_stack((- np.rad2deg([theta_max, omega_max]), np.rad2deg([theta_max, omega_max])))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "colors = [(0,114/255,178/255), (213/255,94/255,0), (0,158/255,115/255), (240/255,228/255,66/255)]\n",
    "\n",
    "# True ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[0]), alpha=1, vmin=0)\n",
    "\n",
    "# Neural network\n",
    "z = lyapunov_nn.safe_set.reshape(grid.num_points)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[1]), alpha=1, vmin=0)\n",
    "\n",
    "# LQR\n",
    "z = lyapunov_lqr.safe_set.reshape(grid.num_points)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[2]), alpha=1, vmin=0)\n",
    "\n",
    "# SOS\n",
    "z = (values_sos <= cmax_sos).reshape(grid.num_points)\n",
    "ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=(colors[3],), linewidths=2)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[3]), alpha=1, vmin=0)\n",
    "\n",
    "# Plot some trajectories\n",
    "N_traj = 11\n",
    "skip = int(grid.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    x = sub_trajectories[n, 0, :] * np.rad2deg(theta_max)\n",
    "    y = sub_trajectories[n, 1, :] * np.rad2deg(omega_max)\n",
    "    ax.plot(x, y, 'k--', linewidth=0.25)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * np.rad2deg(theta_max), sub_states[:, 1] * np.rad2deg(omega_max), dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=3, headlength=6, color='k')\n",
    "\n",
    "ax.set_aspect(theta_max / omega_max)\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'angle [deg]')\n",
    "ax.set_ylabel(r'angular velocity [deg/s]')\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in colors]    \n",
    "legend = ax.legend(proxy, [r'$\\mathcal{S}_\\pi$', r'NN', r'LQR', r'SOS'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "# Plot safe growth over the iterations\n",
    "ax = plt.subplot(222)\n",
    "ax.plot(c_max, '.-', color=colors[1])\n",
    "ax.set_ylabel(r'safe level $c_k$')\n",
    "ax.set_ylim([0, None])\n",
    "plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "roa_fraction = roa.sum() / roa.size\n",
    "ax = plt.subplot(224)\n",
    "ax.plot(np.array(safe_set_fraction) / roa_fraction, '.-', color=colors[1])\n",
    "ax.set_ylabel(r'fraction of $S_\\pi$')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlabel(r'safe level set update iteration $k$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
